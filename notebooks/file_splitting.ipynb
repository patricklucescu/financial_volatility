{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "round-arthritis",
   "metadata": {},
   "source": [
    "This notebook is intended to extract the csv files and split them into individual files for each day. All these files will be stored in data/data_per_day folder and each file name will be of the form ``SPY_20200203.csv``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "appointed-detail",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraries\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import gzip\n",
    "import shutil\n",
    "from timeit import default_timer as timer\n",
    "# Paths\n",
    "sys.path.append(os.path.join(Path(os.getcwd()).parent))  \n",
    "data_path = os.path.join(os.path.join(Path(os.getcwd()).parent), 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sufficient-fitting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/Dropbox/Projects/financial_volatility/financial_volatility/data/ezu_sep2019.gz'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get data location\n",
    "zip_files = list()\n",
    "zip_files = [f for f in os.listdir(data_path) if os.path.isfile(os.path.join(data_path, f))]\n",
    "zip_files = [file for file in zip_files if '.gz' in file]\n",
    "zip_files = np.sort([os.path.join(data_path, x) for x in zip_files])\n",
    "zip_files[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "weird-pennsylvania",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file = zip_files[-1]\n",
    "with gzip.open(zip_file, 'rb') as f_in:\n",
    "        with open(os.path.join(data_path, 'test.csv'), 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "# get the csv file now\n",
    "csv_file = os.path.join(data_path, 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "tropical-summer",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 20190903 in 1.9755029500011005 seconds\n",
      "Finished 20190904 in 1.0672661569988122 seconds\n",
      "Finished 20190905 in 1.57257854299678 seconds\n",
      "Finished 20190906 in 1.1002267040021252 seconds\n",
      "Finished 20190909 in 0.9160565749989473 seconds\n",
      "Finished 20190910 in 1.368200662000163 seconds\n",
      "Finished 20190911 in 0.8929501570019056 seconds\n",
      "Finished 20190912 in 2.421476937000989 seconds\n",
      "Finished 20190913 in 1.16597218500101 seconds\n",
      "Finished 20190916 in 1.0788180349991308 seconds\n",
      "Finished 20190917 in 1.0906696740057669 seconds\n",
      "Finished 20190918 in 1.1572848639989388 seconds\n",
      "Finished 20190919 in 0.8219099639973138 seconds\n",
      "Finished 20190920 in 1.1497841509990394 seconds\n",
      "Finished 20190923 in 0.8277127469991683 seconds\n",
      "Finished 20190924 in 2.146892201999435 seconds\n",
      "Finished 20190925 in 1.5521733180066803 seconds\n",
      "Finished 20190926 in 2.0990242449988727 seconds\n",
      "Finished 20190927 in 2.217232970004261 seconds\n",
      "Finished 20190930 in 1.1520770290007931 seconds\n",
      "CPU times: user 26 s, sys: 1.72 s, total: 27.8 s\n",
      "Wall time: 27.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_parser = pd.read_csv(csv_file, chunksize=5*10**4)\n",
    "start=timer()\n",
    "for df in data_parser:\n",
    "    #print(1)\n",
    "    dates = np.sort(df.DATE.unique())\n",
    "    if 'current_date' not in locals():\n",
    "        current_date = dates[0]\n",
    "    if 'ticker' not in locals():\n",
    "        ticker = df.SYM_ROOT.values[0]\n",
    "    \n",
    "    if len(dates)==1 and current_date==dates[0]:  # only one date which is the current one\n",
    "        if 'df_date' not in locals():\n",
    "            df_date = df \n",
    "        else:\n",
    "            df_date = pd.concat([df_date, df])\n",
    "    elif len(dates)==1 and current_date!=dates[0]:  # only one date but it is not the current date\n",
    "        df_date.DATE = df_date.DATE.astype(str).apply(lambda x: x[:4]+'-'+x[4:6]+'-'+x[6:])\n",
    "        df_date['DT'] = df_date.DATE + ' ' +df_date.TIME_M\n",
    "        df_date.drop(columns=['DATE', 'TIME_M'], inplace=True)\n",
    "        df_date.to_csv(os.path.join(data_path, 'data_per_day',f'{ticker}', f'{ticker}_{current_date}.csv'), index=False)\n",
    "        end=timer()\n",
    "        print(f'Finished {current_date} in {end-start} seconds')\n",
    "        start=timer()\n",
    "        df_date = df\n",
    "        current_date = dates[0]\n",
    "    elif len(dates)==2:\n",
    "        df_date = pd.concat([df_date, df[df.DATE == current_date]])\n",
    "        df_date.DATE = df_date.DATE.astype(str).apply(lambda x: x[:4]+'-'+x[4:6]+'-'+x[6:])\n",
    "        df_date['DT'] = df_date.DATE + ' ' +df_date.TIME_M\n",
    "        df_date.drop(columns=['DATE', 'TIME_M'], inplace=True)\n",
    "        df_date.to_csv(os.path.join(data_path, 'data_per_day',f'{ticker}', f'{ticker}_{current_date}.csv'), index=False)\n",
    "        end=timer()\n",
    "        print(f'Finished {current_date} in {end-start} seconds')\n",
    "        start=timer()\n",
    "        current_date = dates[1]\n",
    "        df_date = df[df.DATE == current_date]\n",
    "    else:\n",
    "        print('More than two dates are present. Chuncksize is too big!')\n",
    "        break\n",
    "df_date.DATE = df_date.DATE.astype(str).apply(lambda x: x[:4]+'-'+x[4:6]+'-'+x[6:])\n",
    "df_date['DT'] = df_date.DATE + ' ' +df_date.TIME_M\n",
    "df_date.drop(columns=['DATE', 'TIME_M'], inplace=True)\n",
    "df_date.to_csv(os.path.join(data_path, 'data_per_day',f'{ticker}', f'{ticker}_{current_date}.csv'), index=False) # as the last chuck is processed but never saved\n",
    "end=timer()\n",
    "print(f'Finished {current_date} in {end-start} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-sector",
   "metadata": {},
   "source": [
    "Several cases:\n",
    "    - only one date which is the current one\n",
    "        - either create or append to df_date\n",
    "    - only one date but it is not the current date\n",
    "        - save the df_date to csv and redifine df_date = df\n",
    "    - two dates:\n",
    "        - truncate, append & save & redifine df_date =  df and the current date\n",
    "    - more than two dates:\n",
    "        - chunck is too big \n",
    "        \n",
    "   Only issue remaining is the last chuck! which you do not save"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
