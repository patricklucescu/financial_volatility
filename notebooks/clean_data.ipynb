{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43e9d8d1",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ba4ee8",
   "metadata": {},
   "source": [
    "This notebook executes the data cleaning part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "assumed-strategy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Required libraries\n",
    "# Required libraries\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import zipfile \n",
    "from timeit import default_timer as timer\n",
    "import sqlalchemy as db\n",
    "# Paths\n",
    "sys.path.append(os.path.join(Path(os.getcwd()).parent))  \n",
    "data_path = os.path.join(os.path.join(Path(os.getcwd()).parent), 'data')\n",
    "data_per_day_path = os.path.join(os.path.join(Path(os.getcwd()).parent), 'data','data_per_day')\n",
    "\n",
    "\n",
    "# create connection to sqlite database\n",
    "db_path = os.path.join(data_path, 'database.db')\n",
    "db_engine = db.create_engine('sqlite:///' + db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alive-piano",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/data/Dropbox/Projects/financial_volatility/financial_volatility/data/data_per_day/SPY',\n",
       " '/data/Dropbox/Projects/financial_volatility/financial_volatility/data/data_per_day/EZU',\n",
       " '/data/Dropbox/Projects/financial_volatility/financial_volatility/data/data_per_day/VEA',\n",
       " '/data/Dropbox/Projects/financial_volatility/financial_volatility/data/data_per_day/EEM',\n",
       " '/data/Dropbox/Projects/financial_volatility/financial_volatility/data/data_per_day/IWM']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the data folders file now\n",
    "data_folders = [f for f in os.listdir(data_per_day_path) if not os.path.isfile(os.path.join(data_per_day_path, f))]\n",
    "data_folders = [file for file in data_folders if '.' not in file]\n",
    "data_folders = [os.path.join(data_per_day_path, x) for x in data_folders]\n",
    "data_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "illegal-cotton",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the csv file now\n",
    "data_folder = data_folders[1]\n",
    "table_name = data_folder[-3:]\n",
    "csv_files = [f for f in os.listdir(data_folder) if os.path.isfile(os.path.join(data_folder, f))]\n",
    "csv_files = [file for file in csv_files if '.csv' in file]\n",
    "csv_files = np.sort([os.path.join(data_folder, x) for x in csv_files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "objective-custom",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EZU'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_folder[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac23a2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(csv_file):\n",
    "    data_df = pd.read_csv(csv_file)\n",
    "    data_df.DT = pd.to_datetime(data_df.DT)\n",
    "    data_df.sort_values(by=['DT'], inplace=True)\n",
    "    data_df.index = data_df.DT\n",
    "    data_df.drop(columns=['DT'],inplace=True)\n",
    "    data_df = data_df.between_time('9:30', '16:00')\n",
    "    data_df.reset_index(drop=False, inplace=True)\n",
    "\n",
    "    # non zero quotes\n",
    "    data_df = data_df.loc[(data_df.BID>0) & (data_df.BIDSIZ>0) & (data_df.ASK>0) & (data_df.ASKSIZ>0)]\n",
    "\n",
    "    # autoselect exchange\n",
    "    data_df['total_size'] = data_df.BIDSIZ + data_df.ASKSIZ\n",
    "    #data_df = data_df.loc[data_df.EX == data_df.groupby(['EX']).sum().total_size.idxmax()]\n",
    "\n",
    "    # delete negative spreads\n",
    "    data_df = data_df.loc[data_df.ASK > data_df.BID]\n",
    "\n",
    "    # mergeQuotesSameTimestamp\n",
    "    ex = data_df.EX.values[0]\n",
    "    sym_root = data_df.SYM_ROOT.values[0]\n",
    "    data_df.drop(columns=['SYM_SUFFIX', 'total_size'], inplace=True)\n",
    "    data_df = data_df.groupby(['DT']).median()\n",
    "    data_df['EX'] = ex\n",
    "    data_df['SYM_ROOT'] = sym_root\n",
    "    data_df.reset_index(drop=False, inplace=True)\n",
    "\n",
    "    # remove entries with spread > 50 * daily median spread\n",
    "    data_df['SPREAD'] = data_df.ASK - data_df.BID\n",
    "    data_df = data_df.loc[data_df['SPREAD'] < 50 * data_df['SPREAD'].median()]\n",
    "\n",
    "    # remove outliers using the centered rolling window approach \n",
    "    def compute_diff(x):\n",
    "        return x.values[window] - np.median(np.delete(x.values,window))\n",
    "\n",
    "    window = 25\n",
    "    data_df.sort_values(by=['DT'], inplace=True)\n",
    "    data_df['SPREAD_DIFF'] = data_df.SPREAD.rolling(2*window+1, min_periods=2*window+1, center=True).apply(compute_diff)\n",
    "    data_df = data_df.loc[(data_df['SPREAD_DIFF'] < 10 * data_df['SPREAD_DIFF'].mean()) | (data_df['SPREAD_DIFF'].isna())]\n",
    "    data_df = data_df.reset_index(drop=True)\n",
    "    \n",
    "    # resample data to 15 minute level\n",
    "    data_df.set_index(['DT'], inplace=True)\n",
    "    data_df[\"MID\"] = data_df.apply(lambda x: (x.ASK * x.ASKSIZ + x.BID * x.BIDSIZ) / (x.ASKSIZ + x.BIDSIZ), axis=1)\n",
    "    data_df = data_df[['MID', 'SYM_ROOT']]\n",
    "    df_resampled = data_df.resample('15min').ffill()\n",
    "    df_resampled = df_resampled.append(pd.DataFrame(data_df[-1:].values, \n",
    "                                                    index=[df_resampled.index[-1] + datetime.timedelta(minutes=15)],columns=data_df.columns)) # get last observation that is not added by ffill\n",
    "\n",
    "    # set new index and forward fill the price data\n",
    "    df_resampled = df_resampled.iloc[1:,:] # observation at 9:30 is going to be NA\n",
    "    new_index = pd.date_range(start=df_resampled.index[0].replace(hour=9, minute=45, second=0), periods=26, freq='15min') # index from 9:45 until 16:00\n",
    "    df_resampled = df_resampled.reindex(new_index, method='ffill')\n",
    "    df_resampled.reset_index(drop=False, inplace=True)\n",
    "    \n",
    "    # compute returns \n",
    "    df_resampled['RET'] = df_resampled.MID.pct_change().apply(np.vectorize(lambda x: np.log(1+x)))\n",
    "    df_resampled = df_resampled.iloc[1:,:] # first return is NA\n",
    "    df_resampled.rename(columns={'index': 'DT'}, inplace = True)\n",
    "    return df_resampled[['DT', 'RET']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d605d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 433 ms, sys: 55.1 ms, total: 488 ms\n",
      "Wall time: 7min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from joblib import Parallel, delayed\n",
    "    \n",
    "df_data_all_days = Parallel(n_jobs=14)(delayed(compute_returns)(i) for i in csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb83aad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in df_data_all_days:\n",
    "    df.to_sql(data_folder[-3:], db_engine, index=False, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e54ee2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_names = db.inspect(db_engine).get_table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "572138f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for table_name in table_names:\n",
    "    data = pd.read_sql(f\"select * from {table_name}\", db_engine)\n",
    "    if 'db_returns' in locals():\n",
    "        db_returns[table_name] = data.RET.values\n",
    "    else:\n",
    "        db_returns = pd.DataFrame(data.RET.values, index=data.DT, columns=[table_name])\n",
    "db_returns.reset_index(drop=False, inplace=True)\n",
    "db_returns.to_sql('returns',db_engine, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2364e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "    plt.plot(resample, RV_av,color='#9932CC',linewidth=2)\n",
    "    plt.savefig(os.path.join(results_path, 'RV_resampling_decay'), dpi=400, facecolor='aliceblue', edgecolor='k',bbox_inches='tight')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
